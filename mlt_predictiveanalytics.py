# -*- coding: utf-8 -*-
"""MLT_PredictiveAnalytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PY95siRqPzXyXeCGXz5kxkiepWWW6_DM

# **Proyek Predictive Analytics : Klasifikasi pada Kualitas Pisang**

- Nama: Jessindy Tanuwijaya
- Email: tanjess676@gmail.com
- ID Dicoding: jessdytan

## 1. Import Library/Packages
"""

# Sistem & File Handling
import os                         # Operasi sistem file & direktori
import shutil                     # Menyalin dan memindahkan file
import zipfile                    # Menangani file ZIP (ekstraksi, kompresi)
import io                         # Operasi I/O berbasis memori
from pathlib import Path          # Representasi path file & direktori modern

# Operasi Numerik & Data
import math                       # Fungsi matematika dasar
import numpy as np                # Operasi array, vektor, dan matriks
import pandas as pd               # Manipulasi dan analisis data tabular

# Visualisasi Data
import matplotlib.pyplot as plt   # Visualisasi dasar (plot, histogram, dll.)
import seaborn as sns             # Visualisasi statistik (heatmap, boxplot, dll.)

# Preprocessing
from sklearn.preprocessing import LabelEncoder  # Mengubah label kategorik jadi numerik
from sklearn.preprocessing import StandardScaler  # Standarisasi fitur (mean=0, std=1)

# Model Machine Learning
from sklearn.linear_model import LogisticRegression         # Model klasifikasi linier
from sklearn.neighbors import KNeighborsClassifier          # Algoritma K-NN
from sklearn.ensemble import RandomForestClassifier         # Random Forest (model ensambel)

# Evaluasi Model
from sklearn.metrics import (confusion_matrix,              # Matriks kebingungan
                             precision_score,               # Skor presisi
                             recall_score,                  # Skor recall
                             f1_score,                      # Skor F1
                             accuracy_score,                # Skor akurasi
                             classification_report)         # Laporan klasifikasi lengkap

# Split Data & Optimasi Model
from sklearn.model_selection import train_test_split        # Membagi data latih & uji
from sklearn.model_selection import cross_val_score         # Validasi silang
from sklearn.model_selection import KFold                   # K-Fold cross-validation
from sklearn.model_selection import GridSearchCV            # Pencarian grid hyperparameter
from sklearn.model_selection import RandomizedSearchCV      # Pencarian acak hyperparameter

# Scikit-learn Shortcut
import sklearn as sk                # Shortcut opsional untuk cek versi atau struktur

"""## 2. Data Wrangling

### 2.1 Download and Extract Dataset From Kaggle

Langkah-langkah untuk Mendapatkan kaggle.json:
1. Login ke Kaggle:
  
  Pergi ke Kaggle dan login menggunakan akun kamu. Jika belum punya akun, Anda bisa mendaftar terlebih dahulu.

2. Buka Halaman Akun Anda:
  
  Setelah login, klik ikon profil di sudut kanan atas halaman, kemudian pilih "**My Account**" dari dropdown menu.

3. Scroll ke Bagian API:
  
  Di halaman "Account", scroll ke bawah sampai menemukan bagian yang bernama "API". Di bagian ini ada tombol "**Create New API Token**". Klik tombol tersebut untuk membuat token API baru.

4. Download kaggle.json:

  Setelah klik tombol "Create New API Token", file kaggle.json akan otomatis diunduh ke komputer Anda.File ini berisi dua informasi penting: username dan key, yang akan digunakan untuk mengakses Kaggle API.
"""

from google.colab import files, userdata
# upload kaggle.json
files.upload()

"""Kode di bawah bertujuan untuk menyiapkan kredensial API Kaggle dengan cara memindahkan file kaggle.json ke folder yang tepat di Google Colab, sehingga API Kaggle dapat digunakan untuk mengakses dataset atau model secara otomatis dan aman."""

# Buat folder .kaggle di home directory
os.makedirs('/root/.kaggle', exist_ok=True)

# Pindahkan kaggle.json ke folder tersebut
shutil.move('kaggle.json', '/root/.kaggle/kaggle.json')

# Ubah permission supaya hanya bisa dibaca oleh user
os.chmod('/root/.kaggle/kaggle.json', 600)

"""Setelah menjalankan perintah dibawah ini, file zip dataset akan diunduh ke direktori kerja saat ini di Google Colab. Anda dapat mengekstrak dan menggunakannya untuk proyek Anda."""

!kaggle datasets download -d l3llff/banana

"""Mengekstrak seluruh isi file zip ke dalam folder dataset. Folder ini akan dibuat secara otomatis jika belum ada."""

with zipfile.ZipFile("banana.zip", 'r') as zip_ref:
    zip_ref.extractall("dataset")

"""### 2.2 Assessing Data

Membaca dan mengimpor dataset kualitas pisang.
"""

df = pd.read_csv('/content/dataset/banana_quality.csv')

"""Menampilkan 5 data teratas"""

df.head()

"""Menampilkan informasi gambaran umum dataset"""

df.info()

# Mengetahui berapa banyak baris dan kolom
print("Banyak Baris:",df.shape[0])
print("Banyak Kolom:", df.shape[1])

"""Menampilkan data statistik dataset"""

df.describe()

"""Mengecek data duplikat"""

df.duplicated().sum()

"""Hasil menunjukkan bahwa tidak ada data duplikat dalam dataset ini.

Mengecek data hilang
"""

df.isnull().sum()

"""Tidak ada data yang hilang dalam dataset ini.

## 3. Exploratory Data Analysis (EDA)

### 3.1 Boxplot untuk mengecek outlier
"""

# Pilih hanya kolom numerik
numerical_cols = df.select_dtypes(exclude='object').columns

# Tentukan jumlah baris dan kolom untuk grid
n_cols = 3  # jumlah kolom yang diinginkan
n_rows = math.ceil(len(numerical_cols) / n_cols)  # hitung jumlah baris berdasarkan jumlah kolom

# Buat figure dan axes dengan grid layout
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 3))
axes = axes.flatten()  # Ubah menjadi array 1D untuk iterasi mudah

# Plot setiap boxplot pada subplot masing-masing
for i, col in enumerate(numerical_cols):
    sns.boxplot(x=df[col], ax=axes[i])
    axes[i].set_title(f"Boxplot {col}")

# Hapus subplot kosong jika jumlah variabel tidak pas dengan grid
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""Dapat dilihat bahwa hampir keseluruhan kolom memiliki data outlier, untuk itu kita akan menghapusnya dengan menggunakan metode IQR."""

def delete_outlier_iqr(df):
    df_clean = df.copy()
    for kolom in df_clean.select_dtypes(exclude='object').columns:
        q1 = df_clean[kolom].quantile(0.25)
        q3 = df_clean[kolom].quantile(0.75)
        iqr = q3 - q1
        batas_bawah = q1 - 1.5 * iqr
        batas_atas = q3 + 1.5 * iqr
        # Hanya simpan baris yang nilainya dalam rentang IQR
        df_clean = df_clean[(df_clean[kolom] >= batas_bawah) & (df_clean[kolom] <= batas_atas)]
    return df_clean

# Hapus outlier
df_clean = delete_outlier_iqr(df)

"""Setelah outlier dihapus, berikut adalah banyak baris dan kolom sekarang."""

# Mengetahui berapa banyak baris dan kolom
print("Banyak Baris:",df_clean.shape[0])
print("Banyak Kolom:", df_clean.shape[1])

"""Karena tidak banyak baris yang dihapus (masih dibawah 30%), maka tindakan penghapusan baris yang memiliki data outlier sudah tepat.

### 3.2 Distribusi Data Numerikal
"""

df_clean.hist(bins=50, figsize=(20,15))
plt.show()

"""Dari hasil visualisasi, semua distribusi terlihat normal, kecuali kolom Softness yang memiliki distribusi bimodal. Dengan adanya distribusi bimodal akan membingungkan model machine learning. Untuk itu, sebaiknya kolom Softness dibuang nantinya.

### 3.3 Heatmap Kolerasi
"""

# Heatmap korelasi
df_clean.select_dtypes(exclude='object').corr()
plt.figure(figsize=(15, 12))
sns.heatmap(df_clean.select_dtypes(exclude='object').corr(), annot=True, cmap='coolwarm',center=0)
plt.title("Correlation Matrix of Numerical Data Type")
plt.show()

"""Dari heatmap, hubungan antar variabel cukup renggang dan tidak ada hubungan yang sangat kuat.

### 3.4 Distribusi Kolom Kategorikal (Quality)
"""

plt.figure(figsize=(8, 5))
sns.countplot(data=df_clean, x='Quality', palette='Set2')
plt.title("Distribusi Label Quality")
plt.xlabel("Quality")
plt.ylabel("Jumlah")
plt.show()

"""Distribusi Label cukup merata

## 4. Data Preprocessing

### 4.1 Encoding

Tahap ini dilakukan karena model machine learning hanya bisa memproses data numerik. Kolom Quality masih berupa data kategori (misalnya "Good", "Bad"), jadi harus diubah ke angka (misalnya 1 dan 0) agar bisa digunakan dalam pelatihan model.
"""

enc = LabelEncoder()
df_clean['Quality'] = enc.fit_transform(df_clean['Quality'])

"""### 4.2 Normalisasi

Tahap ini dilakukan untuk menstandarisasi kolom numerik (kecuali Quality) agar semua fitur memiliki skala yang sama. Ini penting karena fitur dengan skala besar bisa mendominasi model. Dengan StandardScaler(), semua nilai dikonversi menjadi distribusi dengan mean = 0 dan standar deviasi = 1, sehingga model bisa belajar lebih efektif.
"""

numerical_cols = df_clean.select_dtypes(exclude=['object']).columns.drop(['Quality'])

# Scale numerical columns
scaler = StandardScaler()
df_scaled = df_clean.copy()
df_scaled[numerical_cols] = scaler.fit_transform(df_scaled[numerical_cols])

"""Berikut adalah gambaran dataset setelah dilakukan normalisasi"""

df_scaled.head()

"""### 4.3 Splitting Data

Tahap ini memisahkan data menjadi fitur (X) dan target (y), lalu membagi data menjadi data latih (train) dan data uji (test).

Kolom Softness dibuang karena distribusinya **bimodal**, yang bisa menunjukkan dua kelompok berbeda dan berpotensi membingungkan model. Dengan menghapusnya, model bisa fokus pada fitur yang lebih konsisten dan informatif.
test_size=0.3 berarti 30% data digunakan untuk pengujian, dan random_state=42 memastikan hasil pembagian tetap sama setiap kali dijalankan.
"""

X = df_scaled.drop(columns=['Quality', 'Softness'])
y = df_scaled['Quality']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""## 5. Modelling

Tujuan model ini dibangun adalah untuk mengklasifikasikan kualitas pisang berdasarkan sifat fisik dan kimiawi nya. Oleh karena itu, digunakan algoritma untuk klasifikasi.

### 5.1 Membangun model dengan Logistic Regression
"""

model_lr = LogisticRegression(max_iter=10000,solver='saga', random_state=42)
model_lr.fit(X_train, y_train)

"""### 5.2 Membangun model dengan KNN"""

model_knn = KNeighborsClassifier(n_neighbors=5)  # Gunakan k=5 sebagai default
model_knn.fit(X_train, y_train)

"""### 5.3 Membangun model dengan Random Forest Classifier"""

model_rf = RandomForestClassifier(n_estimators=100, random_state=42)
model_rf.fit(X_train, y_train)

"""### 5.4 Evaluasi Model Klasifikasi

#### 5.4.1 Prediksi dengan data uji
"""

y_pred_lr = model_lr.predict(X_test)
y_pred_knn = model_knn.predict(X_test)
y_pred_rf = model_rf.predict(X_test)

"""#### 5.4.2 Menghitung Metrik Evaluasi Model"""

# Membuat fungsi untuk evaluasi model
def evaluate_model(name, y_true, y_pred):
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='weighted')
    print(f"ðŸ”˜ {name}")
    print(f"Accuracy: {acc:.4f}")
    print(f"F1-Score: {f1:.4f}\n")
    print("Classification Report:\n", classification_report(y_true, y_pred))
    print("=" * 60)
    print()

evaluate_model("Logistic Regression", y_test, y_pred_lr)
evaluate_model("KNN Classifier", y_test, y_pred_knn)
evaluate_model("Random Forest Classifier", y_test, y_pred_rf)

"""### 5.5 Melakukan Hypertuning menggunakan GridSearchCV

#### 5.5.1 Hypertuning algoritma KNN
"""

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11, 13],  # Jumlah tetangga
    'weights': ['uniform', 'distance'],  # Metode pembobotan
    'metric': ['euclidean', 'manhattan'],  # Jenis metrik jarak
    'p': [1, 2] # Parameter untuk metrik jarak
}

# Grid Search dengan 5-fold Cross-Validation
grid_search_knn = GridSearchCV(
    KNeighborsClassifier(),
    param_grid_knn,
    cv=5,  # 5-fold cross-validation
    scoring='accuracy',
    n_jobs=-1,
    verbose=2
)

# Training model dengan hyperparameter tuning
grid_search_knn.fit(X_train, y_train)

# Best parameters dan hasil evaluasi
print("Parameter terbaik untuk KNN:", grid_search_knn.best_params_)
best_knn = grid_search_knn.best_estimator_

"""#### 5.5.2 Hypertuning algoritma Random Forest"""

param_grid_rf = {
    'n_estimators': [100, 200],         # Jumlah pohon
    'max_depth': [None, 10, 20],        # Kedalaman maksimum pohon
    'min_samples_split': [2, 5],        # Minimum sampel untuk split
    'min_samples_leaf': [1, 2],         # Minimum sampel di daun
    'bootstrap': [True, False]          # Apakah bootstrap digunakan
}

# Grid Search dengan 5-fold cross-validation
grid_search_rf = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid_rf,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2
)

# Melatih model dengan hyperparameter tuning
grid_search_rf.fit(X_train, y_train)

# Menampilkan parameter terbaik dan model terbaik
print("Parameter terbaik untuk Random Forest:", grid_search_rf.best_params_)
best_rf = grid_search_rf.best_estimator_

"""#### 5.5.3 Prediksi hasil hypertuning menggunakan data uji"""

y_pred_knn_best = best_knn.predict(X_test)
y_pred_rf_best = best_rf.predict(X_test)

"""#### 5.5.4 Evaluasi model hasil hypertuning"""

evaluate_model("KNN Classifier  Setelah Tuning", y_test, y_pred_knn_best)
evaluate_model("Random Forest Classifier Setelah Tuning", y_test, y_pred_rf_best)

"""Dapat disimpulkan, bahwa algoritma KNN paling unggul dibanding algoritma lain, dengan akurasi sebesar 0,9530 yang digolongkan sangat akurat."""